[
  {
    "termin": "VPN",
    "opis": "VPN (ang. Virtual Private Network, pol. Wirtualna Sieć Prywatna) to technologia umożliwiająca ustanowienie bezpiecznego, szyfrowanego połączenia przez publiczną sieć, najczęściej Internet. Działa poprzez tworzenie wirtualnego tunelu między urządzeniem użytkownika a serwerem VPN, w którym cały ruch sieciowy jest szyfrowany, co zabezpiecza przesyłane dane przed przechwyceniem i nieuprawnionym dostępem. Korzystanie z VPN powoduje, że adres IP widoczny dla serwisów internetowych jest adresem serwera VPN, nie zaś rzeczywistym adresem użytkownika, co również zwiększa anonimowość i prywatność w sieci.\n\nVPN ma szerokie zastosowanie w środowiskach korporacyjnych, gdzie umożliwia bezpieczny zdalny dostęp pracowników do zasobów sieci firmowej. Ponadto jest popularny wśród użytkowników indywidualnych dbających o prywatność online, ochronę danych w publicznych sieciach Wi-Fi oraz możliwość ominięcia blokad geograficznych domen.\n\nDo najpopularniejszych typów VPN należą: dostęp zdalny (Remote Access VPN) umożliwiający łączenie się z siecią firmową z dowolnego miejsca; site-to-site VPN, łączący różne lokalizacje organizacji; oraz VPN komercyjny oferowany użytkownikom indywidualnym.\n\nTechnologia opiera się na różnych protokołach transmisyjnych zapewniających bezpieczeństwo i wydajność, takich jak:\n- IPsec – popularny w środowiskach korporacyjnych, stosujący kompleksowe szyfrowanie i uwierzytelnianie;\n- OpenVPN – protokół open source, oferujący wysoką elastyczność i bezpieczeństwo;\n- WireGuard – nowoczesny, lekki i szybki protokół z prostą konfiguracją;\n- L2TP/IPSec – łączący tunelowanie L2TP z szyfrowaniem IPSec.\n\nWybierając usługę VPN, trzeba zwrócić uwagę na kilka kluczowych aspektów: stosowany poziom szyfrowania (standardowo AES-256), politykę prywatności i przechowywania logów (godny zaufania dostawca powinien deklarować brak logowania aktywności użytkowników), lokalizację oraz jurysdykcję dostawcy, prędkość i stabilność połączenia oraz kompatybilność z systemami operacyjnymi i urządzeniami końcowymi."
  },
    {
    "termin": "Deepfake",
    "opis": "Deepfake to zaawansowana technologia wykorzystująca sztuczną inteligencję, zwłaszcza głębokie sieci neuronowe (deep learning), do tworzenia realistycznych, lecz fałszywych materiałów multimedialnych, takich jak filmy, zdjęcia czy nagrania audio. Technologia ta pozwala na manipulację wizerunków i głosów osób, tworząc wiarygodne fałszywki, które są trudne do odróżnienia od prawdziwych. Termin pochodzi od połączenia słów „deep learning” (głębokie uczenie) oraz „fake” (fałszywy). \n\nDeepfake’y mogą przedstawiać osoby w nieautentycznych sytuacjach, wygłaszać słowa, których nie powiedziały, lub być wykorzystywane do tworzenia fałszywych komunikatów, co stwarza poważne zagrożenia w postaci oszustw, manipulacji politycznej, dezinformacji oraz naruszeń prywatności. Przestępcy mogą używać deepfake’ów w kampaniach phishingowych, podszywając się pod zaufane osoby, by wyłudzić dane lub pieniądze.\n\nProces tworzenia deepfake bazuje na trenowaniu algorytmów na dużych zbiorach danych audiowizualnych, które uczą się charakterystycznych cech twarzy, mimiki i głosu, a następnie generują zmodyfikowane, fałszywe treści. Obecnie istnieją także narzędzia do wykrywania deepfake’ów, a edukacja i świadomość użytkowników są kluczowe w obronie przed ich negatywnym wpływem."
  },
  {
    "termin": "DDoS",
    "opis": "DDoS (ang. Distributed Denial of Service, pol. Rozproszona Odmowa Usługi) to rodzaj cyberataku wykorzystujący wiele skompromitowanych systemów do jednoczesnego zalania celu falą żądań, co prowadzi do przeciążenia serwera i uniemożliwienia jego normalnego działania. W przeciwieństwie do prostego ataku DoS pochodzącego z jednego źródła, DDoS wykorzystuje rozproszoną sieć tysięcy lub milionów urządzeń tworzących botnet - mogą to być komputery, serwery czy urządzenia IoT zainfekowane złośliwym oprogramowaniem i kontrolowane zdalnie przez serwer dowodzenia. Ataki DDoS dzielą się na trzy kategorie: wolumetryczne (wyczerpujące przepustowość łącza poprzez ogromne ilości danych), protokołowe (atakujące warstwę protokołów sieciowych wykorzystując słabości w TCP, ICMP czy SIP, przykładem jest SYN flood), oraz ataki na warstwę aplikacji (najbardziej wyrafinowane, naśladujące legalne żądania użytkowników i trudne do wykrycia). Skutki udanego ataku obejmują niedostępność usług przekładającą się na utratę przychodów, uszczerbek na reputacji firmy, a także mogą służyć jako zasłona dymna dla innych niebezpiecznych działań jak kradzież danych. Ochrona wymaga wielowarstwowego podejścia: odpowiedniej infrastruktury z nadmiarową przepustowością, systemów wykrywania i łagodzenia ataków analizujących ruch w czasie rzeczywistym, usług ochrony przed DDoS od wyspecjalizowanych dostawców działających jak bufor, sieci CDN zwiększających naturalną odporność, oraz przygotowanego planu reagowania z jasnymi procedurami komunikacji."
  },
    {
    "termin": "Zero Trust",
    "opis": "Zero Trust (ang. Zero Trust Model, pol. Model Zerowego Zaufania) to nowoczesny model bezpieczeństwa sieciowego, który zakłada, że żadna jednostka, urządzenie ani połączenie nie jest domyślnie godne zaufania, niezależnie od tego, czy znajduje się wewnątrz, czy na zewnątrz sieci organizacji. Fundamentalną zasadą jest zasada „nigdy nie ufaj, zawsze weryfikuj”, co oznacza, że każde żądanie dostępu musi być uwierzytelnione, autoryzowane i monitorowane w czasie rzeczywistym. Model Zero Trust powstał jako odpowiedź na wyzwania współczesnego środowiska IT, takie jak rosnąca praca zdalna, wykorzystanie chmur obliczeniowych i mobilność użytkowników, które obaliły tradycyjne podejście bazujące na zabezpieczeniu granic sieci. Zamiast chronić tylko perymetr sieci, Zero Trust koncentruje się na zabezpieczeniu poszczególnych zasobów, użytkowników i urządzeń poprzez ciągłą weryfikację i minimalizację dostępu. Podstawowe zasady modelu obejmują: explicit verification – każdy dostęp musi być podstawowo i wielowymiarowo weryfikowany na podstawie tożsamości, lokalizacji, stanu urządzenia i innych czynników; least privilege access – użytkownicy i systemy posiadają wyłącznie minimalny niezbędny dostęp, ograniczony czasowo i kontekstowo; assume breach – założenie, że naruszenie lub atak jest realne, przez co stosuje się mikrosegmentację sieci i ciągły monitoring zdarzeń dla ograniczenia szkód. Wdrożenie modelu wymaga zaawansowanych technologii uwierzytelniania wieloskładnikowego (MFA), zarządzania tożsamością i dostępem (IAM), mikrosegmentacji sieci, szyfrowania komunikacji end-to-end oraz systemów analizy behawioralnej (UEBA) i adaptacyjnych polityk dostępu opartych na kontekście. Proces ten wymaga także zmiany kultury organizacyjnej oraz szkoleń dla pracowników. Model Zero Trust przynosi istotne korzyści, m.in. znaczące zmniejszenie powierzchni ataku, lepsze zabezpieczenie przed zagrożeniami wewnętrznymi i zewnętrznymi, zwiększenie odporności na ruch boczny (lateral movement) w atakach, a także ułatwienie spełnienia wymogów regulacyjnych dotyczących bezpieczeństwa i ochrony danych osobowych, takich jak RODO czy HIPAA."
  },
  {
    "termin": "Threat Hunting",
    "opis": "Threat Hunting to proaktywne przeszukiwanie sieci, systemów i punktów końcowych w celu wykrycia zagrożeń, które ominęły tradycyjne zabezpieczenia. W przeciwieństwie do reaktywnego podejścia, gdzie organizacja reaguje na alerty generowane przez systemy bezpieczeństwa, threat hunting zakłada aktywne poszukiwanie śladów obecności atakującego jeszcze zanim zdąży wyrządzić szkody. Opiera się na założeniu, że żadne zabezpieczenia nie są w stu procentach skuteczne, a wyrafinowani atakujący potrafią ominąć nawet najbardziej zaawansowane systemy ochrony. Specjaliści zajmujący się threat hunting (threat hunterzy) wykorzystują swoją wiedzę o taktykach, technikach i procedurach stosowanych przez cyberprzestępców do proaktywnego szukania oznak ich obecności. Tradycyjne systemy działają reaktywnie w oparciu o znane wzorce i generują alerty, podczas gdy threat hunting odwraca ten model - analitycy aktywnie poszukują anomalii, nietypowych zachowań i wskaźników kompromitacji, łączą pozornie niezwiązane zdarzenia. Proces zazwyczaj rozpoczyna się od sformułowania hipotezy opartej na wiedzy o aktualnych zagrożeniach lub podatnościach, następnie zbierane są dane z logów systemowych, ruchu sieciowego i punktów końcowych, analizowane ręcznie i automatycznie z wykorzystaniem frameworków jak MITRE ATT&CK katalogujący taktyki atakujących. Threat hunting wymaga narzędzi EDR zapewniających głęboką widoczność, analizy ruchu sieciowego, platform threat intelligence oraz umiejętności analityków posiadających głęboką wiedzę techniczną i myślących jak atakujący. Organizacje prowadzące regularne polowania znacząco skracają czas wykrycia zagrożeń z miesięcy do dni, a każde polowanie dostarcza informacji pozwalających zidentyfikować słabe punkty i doskonalić zabezpieczenia."
  },
 {
    "termin": "Bug Bounty",
    "opis": "Bug Bounty to program oferujący wynagrodzenie za zgłaszanie luk w zabezpieczeniach oprogramowania, aplikacji webowych czy systemów IT. Organizacje prowadzące te programy zapraszają badaczy bezpieczeństwa, nazywanych łowcami bugów, do testowania swoich systemów i nagradzają finansowo za odkrycie i odpowiedzialne zgłoszenie podatności, zanim zostaną wykorzystane przez cyberprzestępców. Koncepcja ta zyskała popularność jako uzupełnienie tradycyjnych metod testowania – zamiast polegać wyłącznie na wewnętrznych zespołach czy audytach, firmy korzystają z wiedzy globalnej społeczności testerów działających w trybie ciągłym. \n\nProgram rozpoczyna się od zdefiniowania zakresu, czyli określenia systemów objętych testami, kategorii podatności akceptowanych w programie oraz dozwolonych podczas testów działań, co chroni zarówno organizację, jak i badaczy. Następnie ustala się system nagród, często warstwowy, gdzie wysokość wynagrodzenia jest uzależniona od krytyczności podatności – np. za krytyczne luki umożliwiające zdalne wykonanie kodu przyznawane są nagrody sięgające dziesiątek tysięcy dolarów, a za mniej poważne błędy symboliczne kwoty. \n\nZgłoszenie luki odbywa się przez platformę dedykowaną programowi, gdzie badacz dołącza dokładny opis błędu, kroki do odtworzenia oraz dowód koncepcji, zachowując zasadę odpowiedzialnego ujawniania, czyli niepublikowania informacji do czasu naprawy podatności przez właściciela systemu. Programy mogą mieć charakter publiczny, dostępny dla wszystkich zainteresowanych, lub prywatny, ograniczony do zaproszonych testerów. Popularne platformy zarządzające programami Bug Bounty to m.in. HackerOne, Bugcrowd oraz Intigriti, które pomagają w komunikacji, ocenie zgłoszeń i rozliczeniach.\n\nGłówne korzyści z programów Bug Bounty to dostęp do szerokiego grona ekspertów o zróżnicowanych specjalizacjach, efektywność kosztowa (płatności tylko za faktyczne zgłoszenia), a także ciągłe monitorowanie i testowanie systemów informatycznych. Do wyzwań należy obsługa i weryfikacja dużej liczby zgłoszeń, szybkie reagowanie na wykryte podatności oraz utrzymanie motywacji i bezpieczeństwa w relacjach z badaczami."
  },
  {
    "termin": "API Security",
    "opis": "API Security (ang. Application Programming Interface Security) to zbiór praktyk, technologii i mechanizmów ochronnych mających na celu zabezpieczenie interfejsów programistycznych przed nieautoryzowanym dostępem, nadużyciami oraz atakami. W dobie intensywnej wymiany danych między aplikacjami webowymi i mobilnymi, API stanowią kluczowe punkty integracji i często są celem ataków, ponieważ umożliwiają dostęp do wrażliwych danych i funkcji systemów. Organizacja OWASP regularnie publikuje listę dziesięciu najważniejszych zagrożeń dla API, w tym Broken Object Level Authorization, Broken Authentication, Excessive Data Exposure, Lack of Resources & Rate Limiting, Mass Assignment oraz Security Misconfiguration.\n\nPodstawy bezpieczeństwa API obejmują: silne uwierzytelnianie i autoryzację oparte na standardach takich jak OAuth 2.0 oraz OpenID Connect, szyfrowanie komunikacji przez HTTPS z dodatkowymi warstwami zabezpieczeń, walidację danych wejściowych w celu zapobiegania atakom iniekcyjnym, szczegółowe logowanie wywołań API dla potrzeb detekcji i analizy incydentów, a także mechanizmy rate limiting i throttling ograniczające liczbę żądań w określonym czasie. Regularne testy penetracyjne, łączące automatyczne i manualne metody, są niezbędnym elementem utrzymania bezpieczeństwa.\n\nW architekturze bezpieczeństwa API często stosuje się API Gateway jako pojedynczy, centralny punkt wejścia, który umożliwia standaryzację polityk zabezpieczających, uwierzytelnianie, autoryzację oraz zaawansowane monitorowanie i wykrywanie anomalii, co znacznie upraszcza zarządzanie oraz zwiększa stabilność i odporność systemów na ataki."
  },
 {
    "termin": "DevSecOps",
    "opis": "DevSecOps to rozszerzenie metodyki DevOps, integrujące praktyki bezpieczeństwa na każdym etapie cyklu życia oprogramowania – od planowania przez rozwój i testowanie, aż po wdrożenie i utrzymanie. W odróżnieniu od tradycyjnego podejścia, gdzie bezpieczeństwo bywało ostatnim etapem sprawdzania produktu, DevSecOps uznaje ochronę za wspólną odpowiedzialność zespołów deweloperskich, operacyjnych i bezpieczeństwa. Filozofia DevSecOps opiera się na automatyzacji procesów zabezpieczeń, takich jak statyczna analiza kodu (SAST), dynamiczne testy bezpieczeństwa aplikacji (DAST), analiza zależności (Software Composition Analysis), oraz ciągłe monitorowanie zagrożeń w środowisku produkcyjnym. \n\nKluczowym aspektem jest podejście \"Shift Left Security\", które przesuwa testy bezpieczeństwa jak najwcześniej w procesie tworzenia oprogramowania, co umożliwia wcześniejsze wykrywanie i usuwanie podatności, zmniejszając koszty naprawy i zwiększając bezpieczeństwo końcowego produktu. W praktyce DevSecOps stosuje również zarządzanie bezpieczeństwem jako kod (Security as Code), integrując polityki bezpieczeństwa z automatycznym pipeline'em CI/CD, co pozwala na spójną i powtarzalną kontrolę zgodności. \n\nSpołeczność DevSecOps kładzie duży nacisk na współpracę wszystkich uczestników projektu, rozwijanie kompetencji oraz kulturę bezpieczeństwa, co wspiera skuteczne zarządzanie ryzykiem. Największe wyzwania to zmiana kultury organizacyjnej, konieczność szkoleń, inwestycje w narzędzia i integracje oraz wyważenie szybkości dostarczania oprogramowania z wymaganiami bezpieczeństwa. DevSecOps pozwala na szybsze i bezpieczniejsze dostarczanie oprogramowania, zmniejszając ryzyko wystąpienia luk i zapewniając zgodność z regulacjami prawnymi."
  },
  {
    "termin": "Cloud Security",
    "opis": "Cloud Security to zbiór technologii, praktyk i mechanizmów kontrolnych mających na celu ochronę danych, aplikacji i infrastruktury w środowiskach chmurowych. Wraz z rosnącym przenoszeniem zasobów do chmury, w modelach takich jak IaaS, PaaS i SaaS, zabezpieczenie tych środowisk stało się jednym z najważniejszych priorytetów cyberbezpieczeństwa. Model odpowiedzialności dzielonej (Shared Responsibility Model) definiuje wyraźny podział obowiązków między dostawcą chmury a klientem: dostawca odpowiada za bezpieczeństwo infrastruktury chmurowej, w tym fizycznej, sieciowej oraz hiperwizorów, natomiast klient – za ochronę danych, tożsamości, aplikacji, systemów operacyjnych oraz konfiguracji sieciowych i zabezpieczeń zapór. \n\nKluczowe aspekty Cloud Security obejmują zarządzanie tożsamością i dostępem (IAM) z realizacją zasady najmniejszych uprawnień i uwierzytelnianiem wieloskładnikowym, szyfrowanie danych w spoczynku i podczas transmisji z odpowiednim zarządzaniem kluczami, zabezpieczenia sieciowe takie jak segmentacja i wirtualne zapory, a także compliance i governance uwzględniające audyty i zgodność z regulacjami takimi jak RODO. Ponadto wykorzystywane są narzędzia typu CASB (Cloud Access Security Broker) zapewniające widoczność i kontrolę nad użytkownikami i usługami chmurowymi, oraz CSPM (Cloud Security Posture Management), automatyzujące wykrywanie błędnych konfiguracji i niezgodności z politykami bezpieczeństwa. \n\nDo największych zagrożeń należą błędne konfiguracje (jak dostęp do publicznych bucketów S3), słabe zarządzanie tożsamością, niechronione API, zagrożenia typu insider threat oraz utrata danych wskutek braku odpowiednich backupów. Skuteczna ochrona wymaga dogłębnego zrozumienia modelu odpowiedzialności, wdrażania automatyzacji zabezpieczeń przez Infrastructure as Code, ciągłego monitoringu zasobów, regularnych audytów konfiguracji oraz stałego szkolenia zespołów w specyfice bezpieczeństwa chmurowego."
  },
  {
    "termin": "Living off the Land",
    "opis": "Living off the Land to wyrafinowana technika ataku, w której cyberprzestępcy wykorzystują legalne narzędzia systemowe i oprogramowanie już obecne w zaatakowanym środowisku do przeprowadzania złośliwych działań. Zamiast wprowadzać własne złośliwe oprogramowanie, które mogłoby zostać wykryte przez rozwiązania antywirusowe, atakujący używają zaufanych programów takich jak PowerShell, WMI (Windows Management Instrumentation), PsExec, certutil czy narzędzi administracyjnych będących naturalną częścią systemu. Strategia ta jest niezwykle skuteczna, ponieważ wykorzystuje narzędzia używane przez administratorów w codziennej pracy, co sprawia, że złośliwa aktywność wtapia się w normalny ruch sieciowy i jest znacznie trudniejsza do wykrycia przez tradycyjne zabezpieczenia oparte na sygnaturach. Projekt LOLBins (Living Off the Land Binaries) kataloguje pliki wykonywalne Windows, skrypty i biblioteki, które mogą być nadużywane przez atakujących do różnych celów. Typowe scenariusze ataków obejmują: używanie PowerShell do pobierania i wykonywania złośliwego kodu z serwera C&C, wykorzystanie WMI do zdalnego wykonywania poleceń na innych maszynach w sieci, nadużycie certutil.exe (narzędzia do zarządzania certyfikatami) do pobierania plików z internetu omijając proxy, użycie bitsadmin do eksfiltracji skradzionych danych, wykorzystanie regedit do ustanawiania persystencji poprzez klucze autostart, czy nadużycie mshta.exe do wykonywania skryptów VBScript i JavaScript. Atakujący często łączą te techniki w łańcuchy ataków, gdzie każdy krok wykorzystuje inne legalne narzędzie. Obrona przed Living off the Land wymaga wykraczającego poza tradycyjne podejście: wdrożenie rozwiązań EDR (Endpoint Detection and Response) monitorujących zachowania i wykrywających nietypowe użycie legalnych narzędzi poprzez analizę kontekstu i wzorców, ograniczenie uprawnień użytkowników zgodnie z zasadą najmniejszych przywilejów, implementacja Application Whitelisting pozwalającego na wykonywanie tylko zatwierdzonych aplikacji, szczegółowe logowanie wykonywania skryptów (PowerShell Script Block Logging, Command Line Process Auditing), segmentacja sieci ograniczająca lateral movement, monitorowanie nietypowych połączeń sieciowych inicjowanych przez procesy systemowe, oraz regularne szkolenia zespołów IT i bezpieczeństwa w rozpoznawaniu podejrzanych wzorców. Kluczowe wyzwanie polega na odróżnieniu legalnego użycia administracyjnego od złośliwej aktywności, co wymaga głębokiej wiedzy o normalnych wzorcach zachowań w danym środowisku oraz zaawansowanych technik behavioral analytics."
  },
  {
    "termin": "Fileless Malware",
    "opis": "Fileless Malware to typ złośliwego oprogramowania, które działa bez zapisywania plików wykonywalnych na dysku twardym zaatakowanego systemu - istnieje wyłącznie w pamięci RAM. Ta technika sprawia, że malware jest praktycznie niewidoczny dla tradycyjnych rozwiązań antywirusowych, które skanują pliki na dysku w poszukiwaniu znanych sygnatur złośliwego kodu. Fileless malware wykorzystuje legalne procesy systemowe i narzędzia już obecne w systemie operacyjnym (często w połączeniu z technikami Living off the Land), co czyni go szczególnie trudnym do wykrycia i analizy. Ataki tego typu znacząco wzrosły w ostatnich latach, stając się preferowaną metodą wyrafinowanych grup cyberprzestępczych i aktorów APT (Advanced Persistent Threat) ze względu na wysoką skuteczność i niskie ryzyko wykrycia. Mechanizm działania zwykle rozpoczyna się od exploit kita wykorzystującego podatność przeglądarki lub phishingu z makrami Office, które dostarczają początkowy kod bezpośrednio do pamięci bez zapisywania na dysku. Następnie wykorzystywane są narzędzia takie jak PowerShell, WMI, Windows Registry, JavaScript w przeglądarkach czy makra Office do wykonywania złośliwych operacji całkowicie w pamięci RAM. Fileless malware może kraść dane uwierzytelniające z pamięci procesów (credential dumping), ustanawiać backdoory dla zdalnego dostępu, eksfiltrować wrażliwe dane, instalować dodatkowe payloady i rozprzestrzeniać się lateralnie w sieci - wszystko bez zostawiania tradycyjnych śladów na dysku. Po restarcie komputera malware znika z pamięci, ale często wykorzystuje mechanizmy persystencji zapisane w rejestrze Windows, zaplanowanych zadaniach (Scheduled Tasks) lub skryptach uruchamianych przy starcie systemu, aby powrócić po ponownym włączeniu. Cykl życia takiego ataku może trwać miesiącami bez wykrycia. Wykrywanie fileless malware wymaga nowoczesnych rozwiązań EDR (Endpoint Detection and Response) monitorujących zachowania i aktywność w pamięci operacyjnej, a nie tylko skanujących pliki. Obrona obejmuje: behavioral analysis analizującą nietypowe zachowania procesów i wzorce działania, memory scanning - regularne skanowanie zawartości pamięci RAM w poszukiwaniu złośliwych artefaktów, ograniczenie lub całkowite wyłączenie makr w dokumentach Office dla użytkowników niebędących programistami, wyłączenie niepotrzebnych skryptów systemowych i ograniczenie PowerShell Execution Policy, aktualizacje systemów i aplikacji łatające exploity, wdrożenie Application Whitelisting pozwalającego tylko na uruchamianie zatwierdzonych aplikacji, monitorowanie rejestrów systemowych i zaplanowanych zadań w poszukiwaniu podejrzanych wpisów, oraz implementacja mechanizmów ochrony pamięci jak EMET (Enhanced Mitigation Experience Toolkit) czy wbudowane w Windows Defender Exploit Guard."
  },{
    "termin": "Ransomware-as-a-Service (RaaS)",
    "opis": "Ransomware-as-a-Service (RaaS) to model biznesowy w cyberprzestępczości, umożliwiający sprzedaż lub dzierżawę oprogramowania ransomware osobom trzecim, zwanym podmiotami stowarzyszonymi. Model ten działa podobnie do legalnego Software-as-a-Service (SaaS), gdzie dostęp do oprogramowania ransomware uzyskuje się za pomocą subskrypcji lub umowy partnerskiej. Operatorzy RaaS tworzą i utrzymują oprogramowanie, zarządzają infrastrukturą do jego wdrażania oraz przetwarzania płatności, podczas gdy podmioty stowarzyszone wykonują ataki na wybrane cele.\n\nDzięki temu modelowi osoby bez zaawansowanych umiejętności technicznych mogą przeprowadzać skuteczne kampanie ransomware, obniżając próg wejścia do cyberprzestępczego świata. Operatorzy RaaS często działają w ramach zorganizowanych grup z podziałem ról, takich jak programiści, testerzy i negocjatorzy. Modele przychodów obejmują subskrypcje, opłaty jednorazowe, programy partnerskie oraz udział w zyskach.\n\nRaaS jest jednym z głównych powodów gwałtownego wzrostu liczby ataków ransomware na całym świecie, które prowadzą do szyfrowania danych i żądań okupu płaconych często w kryptowalutach. Jest to wysoce wyspecjalizowany, zyskowny i trudny do zwalczania model zagrożenia cybernetycznego."
  }
  {
    "termin": "Supply Chain Attack",
    "opis": "Supply Chain Attack to cyberatak na organizację realizowany poprzez kompromitację mniej zabezpieczonych dostawców, partnerów, podwykonawców lub komponentów w łańcuchu dostaw. Zamiast bezpośrednio atakować dobrze chronioną organizację docelową, cyberprzestępcy infiltrują słabsze ogniwo w ekosystemie biznesowym lub technologicznym, a następnie wykorzystują zaufane relacje, integracje i zależności do penetracji głównego celu. Ataki te są szczególnie niebezpieczne, ponieważ wykorzystują zaufanie między organizacjami, często pozostają niewykryte przez długi czas, a jeden skompromitowany dostawca może prowadzić do naruszenia setek lub tysięcy organizacji jednocześnie. Słynne przykłady to atak SolarWinds (2020), gdzie zaawansowana grupa hakerska zainfekował aktualizacje oprogramowania Orion do monitorowania IT, które następnie zostało zainstalowane przez około 18000 organizacji włączając agencje rządowe USA, oraz atak na Kaseya (2021) kompromitujący narzędzie VSA do zarządzania IT używane przez dostawców usług MSP, co doprowadziło do zaszyfrowania ransomware około 1500 firm. Supply chain attacks dzielą się na kilka kategorii: kompromitacja oprogramowania (wstrzykiwanie złośliwego kodu do legalnych aplikacji podczas procesu budowania, kompromitacja serwerów aktualizacji, zatruwanie repozytoriów kodu), ataki na sprzęt (implantowanie backdoorów lub złośliwych chipów w komponentach hardware podczas produkcji lub dystrybucji), kompromitacja usług (atakowanie dostawców SaaS, cloud service providers lub managed service providers aby uzyskać dostęp do ich klientów), ataki na open source (wstrzykiwanie złośliwego kodu do popularnych bibliotek i pakietów wykorzystywanych przez tysiące projektów), oraz ataki na infrastrukturę budowania i dostarczania (kompromitacja systemów CI/CD, serwerów build, repozytoriów artefaktów). Skutki są często katastrofalne i trudne do oszacowania ze względu na efekt kaskadowy - naruszenie jednego dostawcy może propagować się przez całą sieć partnerów i klientów. Ochrona przed supply chain attacks wymaga kompleksowego podejścia: due diligence dostawców (szczegółowe audyty bezpieczeństwa partnerów i dostawców przed nawiązaniem współpracy, ciągła ocena ich poziomu zabezpieczeń), monitorowanie integralności oprogramowania (weryfikacja podpisów cyfrowych, sprawdzanie sum kontrolnych, wykrywanie nieautoryzowanych modyfikacji), izolacja i segmentacja (ograniczenie dostępu dostawców tylko do niezbędnych systemów, implementacja zasady zero trust dla wszystkich połączeń zewnętrznych), Software Bill of Materials - SBOM (szczegółowa inwentaryzacja wszystkich komponentów oprogramowania i ich źródeł), monitorowanie łańcucha dostaw (ciągłe śledzenie zmian u dostawców, alertowanie o incydentach bezpieczeństwa w ekosystemie), implementacja bezpiecznego procesu aktualizacji (testowanie aktualizacji w izolowanym środowisku przed wdrożeniem produkcyjnym, stopniowe rollout), threat intelligence (śledzenie informacji o aktywnych kampaniach atakujących supply chain), oraz plany ciągłości biznesowej uwzględniające scenariusz kompromitacji kluczowych dostawców. Organizacje muszą również rozważyć ryzyko koncentracji - nadmierne poleganie na pojedynczych dostawcach zwiększa potencjalny wpływ ich kompromitacji."
  },
  {
    "termin": "Shadow IT",
    "opis": "Shadow IT to zjawisko polegające na wykorzystywaniu nieautoryzowanych systemów informatycznych, aplikacji, usług chmurowych lub urządzeń przez pracowników bez wiedzy, zgody lub nadzoru działu IT organizacji. Shadow IT powstaje gdy pracownicy, chcąc zwiększyć swoją produktywność lub obejść ograniczenia oficjalnych systemów, samodzielnie wdrażają rozwiązania technologiczne bez przechodzenia przez formalne procesy zatwierdzania i wdrażania. W erze cyfrowej transformacji i łatwego dostępu do usług SaaS, problem Shadow IT nasilił się dramatycznie - badania pokazują, że organizacje mogą mieć świadomość tylko około 30-40% rzeczywiście używanych aplikacji i usług. Typowe przykłady Shadow IT obejmują: wykorzystywanie osobistych kont Dropbox, Google Drive czy OneDrive do przechowywania i udostępniania firmowych dokumentów, używanie niezatwierdzonych komunikatorów (WhatsApp, Telegram) do rozmów służbowych, instalowanie nieautoryzowanego oprogramowania na służbowych komputerach, korzystanie z osobistych urządzeń (BYOD - Bring Your Own Device) do pracy bez odpowiednich zabezpieczeń, czy wykorzystywanie darmowych narzędzi online do analizy danych lub współpracy. Przyczyny powstawania Shadow IT są różnorodne: zbyt długie procesy zatwierdzania oficjalnych rozwiązań, brak odpowiednich narzędzi dostarczanych przez dział IT, ograniczenia funkcjonalne zatwierdzonych aplikacji, lepsza użyteczność niezatwierdzonych rozwiązań, lub po prostu brak świadomości pracowników o ryzyku i politykach bezpieczeństwa. Zagrożenia związane z Shadow IT są poważne: utrata kontroli nad danymi firmowymi (wrażliwe informacje przechowywane poza zabezpieczoną infrastrukturą), brak zgodności z regulacjami (naruszenia RODO, wymogów branżowych), zwiększone ryzyko naruszeń bezpieczeństwa (niezałatowane podatności, słabe hasła, brak MFA), brak kopii zapasowych i disaster recovery, problemy z integracją systemów, niemożność audytu i forensyki po incydentach, oraz potencjalne koszty licencyjne i prawne. Zarządzanie Shadow IT wymaga zbalansowanego podejścia łączącego kontrolę z elastycznością: wdrożenie Cloud Access Security Broker (CASB) wykrywającego i monitorującego nieautoryzowane usługi chmurowe, regularne audyty wykorzystywanych aplikacji i usług, edukacja pracowników o ryzyku i dostępnych autoryzowanych alternatywach, uproszczenie procesów zatwierdzania nowych narzędzi, proaktywne dostarczanie funkcjonalnych i przyjaznych użytkownikowi rozwiązań spełniających potrzeby biznesowe, implementacja polityk akceptowalnego użycia jasno komunikowanych całej organizacji, oraz współpraca między IT a biznesem w zrozumieniu rzeczywistych potrzeb. Zamiast całkowicie blokować Shadow IT (co często jest niemożliwe i prowadzi do jeszcze większej kreatywności pracowników w obchodzeniu ograniczeń), organizacje powinny dążyć do jego identyfikacji, oceny ryzyka i selektywnej legalizacji najbezpieczniejszych rozwiązań spełniających potrzeby użytkowników."
  },
  {
    "termin": "Data Loss Prevention",
    "opis": "Data Loss Prevention (DLP, pol. Zapobieganie Utracie Danych) to zestaw technologii, narzędzi i procedur mających na celu zapobieganie nieautoryzowanemu udostępnianiu, przesyłaniu lub niszczeniu wrażliwych danych organizacji. Systemy DLP monitorują, wykrywają i blokują potencjalne naruszenia polityk bezpieczeństwa poprzez analizę przepływu danych w organizacji - zarówno w spoczynku (na dyskach, w bazach danych), w ruchu (przez sieć, email, komunikatory) jak i w użyciu (przetwarzane przez aplikacje i użytkowników). W dobie rosnących wymagań regulacyjnych (RODO, HIPAA, PCI DSS) oraz zwiększonej mobilności pracowników i danych, DLP stał się kluczowym elementem strategii ochrony informacji. Rozwiązania DLP działają w oparciu o różne metody identyfikacji wrażliwych danych: content inspection (analiza treści dokumentów, emaili, komunikacji w poszukiwaniu wzorców jak numery kart kredytowych, PESEL, dane medyczne), context analysis (analiza kontekstu - kto, gdzie, kiedy i w jaki sposób próbuje uzyskać dostęp do danych), oraz fingerprinting (tworzenie unikalnych odcisków palców dla konkretnych dokumentów pozwalających śledzić ich kopie). Systemy DLP można podzielić na trzy główne typy: Network DLP (monitoruje ruch sieciowy wychwytując próby wysłania wrażliwych danych przez email, web upload, komunikatory), Endpoint DLP (działa na urządzeniach końcowych kontrolując próby skopiowania danych na USB, wydrukowania dokumentów, przesłania przez niezatwierdzone aplikacje), oraz Cloud DLP (chroni dane w aplikacjach SaaS i środowiskach chmurowych). Typowe scenariusze zastosowania obejmują: zapobieganie przypadkowemu wysłaniu wrażliwych dokumentów do niewłaściwych odbiorców, blokowanie prób eksfiltracji danych przez niezadowolonych pracowników, ochrona własności intelektualnej przed kradzieżą, zapewnienie zgodności z regulacjami poprzez automatyczne egzekwowanie polityk, oraz wykrywanie potencjalnych incydentów bezpieczeństwa poprzez nietypowe wzorce dostępu. Implementacja DLP powinna być poprzedzona klasyfikacją danych (określenie co jest wrażliwe i wymaga ochrony), zdefiniowaniem jasnych polityk bezpieczeństwa dopasowanych do potrzeb biznesowych, oraz zaangażowaniem interesariuszy w proces. Wyzwania związane z DLP to: duża liczba fałszywych alarmów wymagających tuningu polityk, potencjalne spowolnienie produktywności przez nadmiernie restrykcyjne zasady, złożoność zarządzania w rozproszonym środowisku, trudność w wykrywaniu zaawansowanych technik obfuskacji danych, oraz konieczność balansowania bezpieczeństwa z użytecznością. Skuteczny program DLP wymaga nie tylko technologii, ale także odpowiedniej kultury organizacyjnej, szkoleń pracowników w zakresie właściwego obchodzenia się z danymi, oraz ciągłego monitorowania i dostosowywania polityk do zmieniających się potrzeb biznesowych i zagrożeń."
  },
  {
    "termin": "Insider Threat",
    "opis": "Insider Threat (pol. Zagrożenie Wewnętrzne) to ryzyko bezpieczeństwa pochodzące od osób mających autoryzowany dostęp do zasobów organizacji - pracowników, kontraktorów, partnerów biznesowych lub byłych pracowników zachowujących dostęp. W przeciwieństwie do zewnętrznych cyberprzestępców, którzy muszą pokonać warstwy zabezpieczeń perimetrycznych, insiderzy posiadają legalne uprawnienia, wiedzę o systemach i zaufanie organizacji, co czyni ich potencjalnie najniebezpieczniejszym typem zagrożenia. Insider threats można podzielić na kilka kategorii: złośliwy insider (celowo szkodzi organizacji z pobudek finansowych, zemsty, szpiegostwa przemysłowego), niedbały insider (nieintencjonalnie powoduje incydenty przez lekkomyślność, ignorowanie polityk bezpieczeństwa, padanie ofiarą phishingu), skompromitowany insider (którego dane uwierzytelniające lub urządzenia zostały przejęte przez zewnętrznego atakującego), oraz insider będący mułem (świadomie lub nieświadomie pomagający zewnętrznym atakującym). Motywacje złośliwych insiderów są różnorodne: zysk finansowy (sprzedaż danych konkurencji lub na czarnym rynku), zemsta (niezadowolenie z oceny pracy, zwolnienie, konflikty z przełożonymi), ideologia lub aktywizm, szpiegostwo przemysłowy lub państwowy, oraz coercion (szantaż, wymuszenie). Typowe działania insiderów obejmują: kradzież własności intelektualnej przed odejściem do konkurencji, sabotaż systemów lub danych, instalowanie backdoorów dla późniejszego lub zewnętrznego dostępu, nadużywanie uprawnień do dostępu do wrażliwych informacji, eksfiltrację danych klientów lub finansowych, oraz modyfikację lub niszczenie krytycznych danych. Insider threats są szczególnie trudne do wykrycia, ponieważ działania insiderów często wyglądają jak normalna aktywność robocza - mają oni uprawnienia do systemów, znają gdzie są cenne dane, rozumieją zabezpieczenia i wiedzą jak je obejść. Wykrywanie wymaga zaawansowanych technik: User and Entity Behavior Analytics (UEBA) wykorzystujących machine learning do wykrywania anomalii w zachowaniach użytkowników, szczegółowego logowania i monitorowania wszystkich operacji na wrażliwych danych (kto, co, kiedy, skąd), analiza wzorców dostępu i eksportu danych, monitoring sygnałów ostrzegawczych (prywatne problemy finansowe, konflikty w pracy, zainteresowanie obszarami poza zakresem obowiązków, nietypowe godziny pracy). Ochrona przed insider threats wymaga wielowarstwowego podejścia: implementacja zasady najmniejszych uprawnień i segregacji obowiązków (separation of duties), okresowa weryfikacja i audyt uprawnień, wdrożenie silnych mechanizmów uwierzytelniania i monitorowania sesji uprzywilejowanych, DLP zapobiegający eksfiltracji danych, kontrola dostępu do wymiennych nośników i urządzeń osobistych, offboarding procesów natychmiast odbierających dostęp odchodzącym pracownikom, programy świadomości bezpieczeństwa edukujące o konsekwencjach, kultura organizacyjna wspierająca zgłaszanie podejrzanych zachowań, oraz programy wellbeing wspierające pracowników w trudnych sytuacjach życiowych. Równie ważne jest utworzenie insider threat program - dedykowanego zespołu łączącego IT, bezpieczeństwo, HR i legal, który ocenia ryzyka i reaguje na incydenty w sposób skoordynowany i zgodny z prawem pracy."
  },
  {
    "termin": "Cyber Resilience",
    "opis": "Cyber Resilience (pol. Odporność Cybernetyczna) to zdolność organizacji do przygotowania się na cyberataki, przeciwstawienia się im, dostosowania do zmieniających się warunków i szybkiego odzyskania funkcjonalności po incydentach, przy jednoczesnym utrzymaniu ciągłości kluczowych operacji biznesowych. W przeciwieństwie do tradycyjnego podejścia do cyberbezpieczeństwa skupiającego się głównie na prewencji i ochronie, cyber resilience przyjmuje realistyczne założenie, że skuteczne naruszenie bezpieczeństwa jest nieuniknione i organizacja musi być przygotowana nie tylko na zapobieganie atakom, ale również na efektywne reagowanie i odbudowę. Koncepcja ta wychodzi poza czysto techniczne aspekty bezpieczeństwa, obejmując ludzi, procesy, technologie oraz strategię biznesową. Cyber resilience składa się z pięciu kluczowych filarów zgodnych z frameworkiem NIST: Identify (identyfikuj - zrozumienie zasobów, danych, ryzyk i środowiska organizacji), Protect (chroń - implementacja odpowiednich zabezpieczeń), Detect (wykrywaj - ciągłe monitorowanie w celu szybkiego wykrywania incydentów), Respond (reaguj - działania mające na celu powstrzymanie i zneutralizowanie incydentów), oraz Recover (odzyskuj - procedury przywracania normalnego działania). Organizacje o wysokiej cyber resilience charakteryzują się kilkoma cechami: posiadają aktualne i regularnie testowane plany ciągłości biznesowej i disaster recovery, przeprowadzają regularne ćwiczenia i symulacje cyberataków (tabletop exercises, red team exercises), mają zdefiniowane i przećwiczone procedury reagowania na incydenty z jasnymi rolami i odpowiedzialnościami, utrzymują aktualne i przetestowane kopie zapasowe krytycznych danych i systemów (zgodnie z zasadą 3-2-1), wdrożyły redundancję krytycznych systemów i możliwość przełączania na alternatywne środowiska, posiadają insurance cybernetyczne pokrywające potencjalne straty, oraz kultywują kulturę ciągłego uczenia się i doskonalenia opartą na analizie incydentów. Budowanie cyber resilience wymaga holistycznego podejścia: oceny ryzyka identyfikującej krytyczne aktywa i scenariusze zagrożeń, inwestycji w technologie wspierające wykrywanie i reagowanie (SIEM, EDR, SOAR), szkoleń i ćwiczeń zespołów w zakresie reagowania na incydenty, współpracy między działami IT, bezpieczeństwa i biznesu w zrozumieniu zależności i priorytetów, testowania planów odzyskiwania w realistycznych warunkach, monitorowania threat intelligence dla wyprzedzającego przygotowania, oraz budowania relacji z zewnętrznymi partnerami (CERT/CSIRT, organy ścigania, firmy forensyczne). Metryki cyber resilience obejmują: Recovery Time Objective (RTO - maksymalny akceptowalny czas przestoju), Recovery Point Objective (RPO - maksymalna akceptowalna utrata danych), Mean Time to Detect (MTTD - średni czas wykrycia incydentu), Mean Time to Respond (MTTR - średni czas reakcji), oraz zdolność do utrzymania określonego poziomu operacji podczas ataku. Cyber resilience to nie jednorazowy projekt, ale ciągły proces wymagający regularnych przeglądów, aktualizacji i doskonalenia w odpowiedzi na ewoluujące zagrożenia i zmieniające się środowisko biznesowe."
  },
  {
    "termin": "Privacy by Design",
    "opis": "Privacy by Design (pol. Prywatność przez Projekt) to podejście do projektowania systemów, produktów i usług, w którym ochrona prywatności i danych osobowych jest wbudowana od samego początku procesu rozwoju, a nie dodawana jako warstwa zabezpieczeń na końcu. Koncepcja ta, sformułowana przez Dr. Ann Cavoukian, opiera się na proaktywnej filozofii, że prywatność powinna być domyślnym stanem systemów, a nie opcją wymagającą dodatkowej konfiguracji. Privacy by Design stała się nie tylko best practice, ale wymogiem prawnym w wielu jurysdykcjach - RODO (Rozporządzenie o Ochronie Danych Osobowych) explicite wymaga stosowania zasad data protection by design and by default. Podejście to składa się z siedmiu fundamentalnych zasad: proaktywność nie reaktywność (antycypowanie i zapobieganie problemom z prywatnością zanim wystąpią, a nie reagowanie po fakcie), prywatność jako domyślne ustawienie (systemy powinny automatycznie chronić dane osobowe bez wymagania od użytkowników dodatkowych działań), prywatność wbudowana w projekt (ochrona danych jako integralna część funkcjonalności, a nie dodatek), pełna funkcjonalność - podejście win-win (prywatność nie wymaga kompromisów z funkcjonalnością, możliwe jest osiągnięcie obu), bezpieczeństwo end-to-end przez cały cykl życia danych (od zbierania po usunięcie), widoczność i przejrzystość (operacje na danych są jawne i weryfikowalne), oraz poszanowanie prywatności użytkownika (systemy user-centric stawiające potrzeby użytkowników na pierwszym miejscu). Praktyczna implementacja Privacy by Design obejmuje szereg technik i praktyk: minimalizacja danych (zbieranie tylko absolutnie niezbędnych informacji do realizacji celu), ograniczenie celów (używanie danych wyłącznie do zadeklarowanych i uzasadnionych celów), anonimizacja i pseudonimizacja (techniczne utrudnianie identyfikacji osób), szyfrowanie danych w spoczynku i w tranzycie, implementacja kontroli dostępu i zasady najmniejszych uprawnień, automatyczne usuwanie danych po upływie okresu retencji, privacy-preserving technologies (np. differential privacy, homomorphic encryption, secure multi-party computation), user consent management (granularne zarządzanie zgodami), transparency mechanisms (jasna komunikacja jak dane są używane), oraz data breach notification systems. Proces projektowania z uwzględnieniem prywatności powinien rozpoczynać się od Privacy Impact Assessment (PIA) lub Data Protection Impact Assessment (DPIA) wymaganego przez RODO dla operacji wysokiego ryzyka - systematycznej analizy jak projekt wpłynie na prywatność i jakie środki zaradcze są potrzebne. Privacy by Design wymaga także zaangażowania Data Protection Officer (DPO) lub privacy specialists w proces od najwcześniejszych etapów, interdyscyplinarnych zespołów łączących deweloperów, architektów, prawników i specjalistów bezpieczeństwa, oraz ciągłego privacy training dla wszystkich zaangażowanych. Korzyści wykraczają poza compliance: organizacje praktykujące Privacy by Design budują zaufanie użytkowników, redukują ryzyko kosztownych naruszeń danych i kar regulacyjnych, zyskują przewagę konkurencyjną na rynkach świadomych prywatności, oraz unikają kosztownych redesignów po odkryciu problemów. Wyzwania obejmują początkowe dodatkowe koszty i czas developmentu, konieczność zmiany kultury organizacyjnej i procesów, oraz trudność w mierzeniu ROI inwestycji w prywatność. Privacy by Design jest szczególnie istotne w kontekście nowych technologii jak AI i machine learning, IoT, biometria czy blockchain, gdzie implikacje dla prywatności mogą być głębokie i trudne do przewidzenia bez odpowiedniego planowania."
  },
  {
    "termin": "Cyber Hygiene",
    "opis": "Cyber Hygiene (pol. Higiena Cybernetyczna) to zestaw podstawowych praktyk, nawyków i rutynowych działań, które użytkownicy indywidualni i organizacje powinni regularnie wykonywać w celu utrzymania bezpieczeństwa cyfrowego i minimalizacji ryzyka cyberataków. Podobnie jak higiena osobista zapobiega chorobom fizycznym, cyber hygiene zapobiega infekcjom cyfrowym i naruszeniom bezpieczeństwa. Koncepcja ta koncentruje się na prostych, ale skutecznych działaniach, które stają się drugą naturą i tworzą solidny fundament ochrony. Podstawowe praktyki cyber hygiene obejmują zarządzanie hasłami: używanie silnych, unikalnych haseł dla każdego konta (minimum 12-16 znaków, kombinacja wielkich i małych liter, cyfr, znaków specjalnych), wykorzystywanie password managera do bezpiecznego przechowywania, włączanie uwierzytelniania wieloskładnikowego wszędzie gdzie dostępne, regularna zmiana haseł do krytycznych kont. Aktualizacje oprogramowania to kolejny krytyczny element: natychmiastowe instalowanie aktualizacji bezpieczeństwa dla systemu operacyjnego, przeglądarek, aplikacji i firmware urządzeń, włączanie automatycznych aktualizacji gdzie możliwe, usuwanie nieużywanego oprogramowania. Bezpieczeństwo sieci obejmuje: używanie VPN w publicznych sieciach Wi-Fi, zmianę domyślnych haseł routerów, włączanie szyfrowania WPA3, wyłączanie WPS. Ochrona urządzeń wymaga: instalacji i aktualizacji oprogramowania antywirusowego, włączenia zapór systemowych, szyfrowania dysków (BitLocker, FileVault), fizycznego zabezpieczenia urządzeń. Bezpieczeństwo email: weryfikacja nadawców przed otwarciem załączników, nieufność wobec linków w niespodziewanych wiadomościach, używanie osobnych adresów do różnych celów. Kopie zapasowe to kluczowa praktyka: regularne backupy według zasady 3-2-1 (3 kopie, 2 różne media, 1 offsite), testowanie odzyskiwania, automatyzacja procesu. Bezpieczeństwo przeglądania: używanie aktualnych przeglądarek, blokery reklam i skryptów, weryfikacja certyfikatów HTTPS, czyszczenie cache i cookies. Zarządzanie prywatnością w social media: kontrola ustawień prywatności, ograniczenie udostępnianych informacji, świadomość phishingu przez media społecznościowe. Dla organizacji cyber hygiene obejmuje także: inwentaryzację wszystkich urządzeń i oprogramowania, zarządzanie cyklem życia sprzętu, audyty uprawnień użytkowników, monitoring logów, regularne szkolenia pracowników, testy świadomości (symulowane phishing), polityki BYOD, procedury offboardingu. Monitorowanie kont: regularne sprawdzanie wyciągów bankowych i aktywności kont, włączanie alertów o podejrzanych działaniach, używanie usług monitoringu dark web. Cyber hygiene to nie jednorazowe działania, ale ciągłe nawyki wymagające dyscypliny i konsekwencji, które znacząco redukują ryzyko padnięcia ofiarą najpowszechniejszych ataków."
  },
{
    "termin": "Środki mitygujące",
    "opis": "Środki mitygujące (ang. Mitigating Controls, Compensating Controls) to zabezpieczenia, procedury lub mechanizmy implementowane w celu zmniejszenia ryzyka związanego z konkretnym zagrożeniem lub podatnością, gdy eliminacja źródła ryzyka nie jest możliwa lub praktyczna. W zarządzaniu ryzykiem, organizacje rzadko mogą wyeliminować wszystkie zagrożenia całkowicie - środki mitygujące pozwalają redukować ryzyko do akceptowalnego poziomu przy rozsądnych kosztach. Działają na różnych poziomach: preventive (zapobiegają incydentowi), detective (wykrywają gdy wystąpi), corrective (naprawiają skutki), deterrent (zniechęcają atakujących), recovery (przywracają funkcjonalność), compensating (kompensują brak innych kontroli). Przykłady dla różnych scenariuszy: dla nieałatowanej krytycznej podatności (virtual patching przez WAF, network segmentation izolująca podatny system, zwiększone monitorowanie, ograniczenie dostępu do minimum), dla braku MFA w legacy systemie (IP whitelisting, strong password policies, monitoring failed logins, session timeouts), dla ryzyka ransomware (regularne offline backups, email filtering, endpoint protection, user training, network segmentation), dla insider threats (separation of duties, audit logging, data loss prevention, background checks, access reviews), dla cloud misconfigurations (Cloud Security Posture Management, Infrastructure as Code z security checks, automated compliance scanning). Wybór odpowiednich środków mitygujących wymaga: oceny ryzyka (likelihood i impact), analizy kosztów implementacji vs potencjalnych strat, zrozumienia attack vectors, compliance requirements, oraz praktyczności w danym środowisku. Efektywne środki mitygujące powinny: adresować root cause lub path ryzyka, być proporcjonalne do poziomu ryzyka, nie tworzyć nowego ryzyka, być monitorowalne i weryfikowalne, oraz mieć jasne ownership. Defense in depth to strategia używania multiple layers środków mitygujących by zapewnić że failure pojedynczej kontroli nie prowadzi do kompromitacji. Organizacje powinny dokumentować accepted risks i implemented mitigations, regularnie review ich efektywności, oraz update w odpowiedzi na zmieniające się zagrożenia i environment."
  },
  {
    "termin": "Cyber Kill Chain",
    "opis": "Cyber Kill Chain to model opisujący fazy cyberataku opracowany przez Lockheed Martin, pierwotnie bazujący na koncepcji wojskowej kill chain. Model ten dzieli atak na siedem sekwencyjnych faz, pomagając organizacjom zrozumieć jak atakujący działają i gdzie mogą implementować obronę by przerwać atak. Fazy Cyber Kill Chain to: Reconnaissance (rekonesans - atakujący zbierają informacje o celu poprzez OSINT, scanning sieci, social engineering, identyfikują podatności i potencjalne punkty wejścia), Weaponization (uzbrojenie - tworzenie exploita i deliverable payload, często łączenie exploita z backdoorem w deliverable package), Delivery (dostarczenie - przesłanie weaponized bundle do ofiary przez email attachments, malicious websites, infected USB), Exploitation (eksploatacja - wykonanie kodu exploita na systemie ofiary wykorzystując podatność aplikacji, OS lub użytkownika), Installation (instalacja - złośliwe oprogramowanie instaluje backdoor lub inne persistence mechanisms na systemie ofiary), Command and Control (dowodzenie i kontrola - malware nawiązuje komunikację z serwerem C2 atakującego pozwalając na remote manipulation), oraz Actions on Objectives (działania na celach - atakujący realizują swój cel: exfiltration danych, destruction, encryption dla ransomware). Kluczowa idea modelu to że obrońcy mogą przerwać atak na każdym z etapów - im wcześniej, tym mniej szkód. Model pomaga w: planowaniu layered defense (różne kontrole dla różnych faz), priorytetyzacji inwestycji w security, incident analysis (określenie jak daleko posunął się atak), threat hunting (proaktywne szukanie wskaźników dla każdej fazy), oraz komunikacji między zespołami bezpieczeństwa. Model ma też ograniczenia - jest liniowy podczas gdy współczesne ataki są często iteracyjne i równoległe, oraz skupia się na external threats pomijając insider threats. Mimo to pozostaje użytecznym frameworkiem dla understanding i disrupting cyberataków."
  },
  {
    "termin": "Inżynieria społeczna",
    "opis": "Inżynieria społeczna to manipulacja psychologiczna ludzi mająca na celu skłonienie ich do wykonania określonych działań lub ujawnienia poufnych informacji. W przeciwieństwie do ataków technicznych wykorzystujących luki w oprogramowaniu, inżynieria społeczna eksploatuje ludzką naturę - zaufanie, chęć pomocy, strach, ciekawość, chciwość czy poczucie pilności. Jest to jedna z najskuteczniejszych metod przeprowadzania cyberataków, ponieważ najsłabszym ogniwem w łańcuchu bezpieczeństwa zazwyczaj są ludzie. Według raportów branżowych, ponad 90% udanych cyberataków zaczyna się od jakiejś formy inżynierii społecznej. Techniki są bardzo różnorodne: phishing (fałszywe emaile podszywające się pod zaufane źródła), spear phishing (ukierunkowany na konkretne osoby po przeprowadzeniu rekonesansu), vishing (voice phishing przez telefon), smishing (phishing przez SMS), pretexting (tworzenie fałszywego scenariusza by wyciągnąć informacje), baiting (oferowanie czegoś kuszącego w zamian za dane lub podrzucanie zainfekowanych USB), tailgating (fizyczne wtargnięcie podążając za autoryzowaną osobą), quid pro quo (oferowanie usługi w zamian za informacje). Psychologiczne zasady wykorzystywane to: reciprocity (wzajemność - czujemy się zobowiązani odwdzięczyć), commitment and consistency (dążymy do bycia konsekwentnymi), social proof (robimy to co inni), authority (ufamy autorytetom), liking (bardziej ufamy osobom które lubimy), scarcity (ograniczona dostępność zwiększa wartość). Obrona wymaga wielowarstwowego podejścia: regularne szkolenia świadomości dla wszystkich pracowników z symulowanymi atakami phishingowymi, weryfikacja tożsamości przy prośbach o wrażliwe dane lub nietypowych żądaniach (call-back procedures), polityki clear desk i clear screen, procedury weryfikacji przy zmianach danych bankowych czy trasferach, kultura organizacyjna zachęcająca do zadawania pytań i zgłaszania podejrzeń bez obawy o konsekwencje, techniczne zabezpieczenia jak filtrowanie emaili, MFA utrudniające wykorzystanie wykradzionych haseł, ograniczenie informacji publicznych o organizacji i pracownikach (OSINT hardening). Kluczowe jest zrozumienie, że inżynieria społeczna nie jest problemem technologicznym, lecz ludzkim, wymagającym ciągłej edukacji i budowania kultury bezpieczeństwa."
  },
  {
    "termin": "Watering Hole Attack",
    "opis": "Watering Hole Attack (pol. Atak Wodopoju) to wyrafinowana strategia cyberataków polegająca na infekowaniu stron internetowych często odwiedzanych przez określoną grupę docelową, zamiast bezpośredniego atakowania samych ofiar. Nazwa pochodzi z analogii do drapieżników czyhających przy wodopojach, gdzie ich ofiary muszą pojawić się by pić. Atakujący najpierw identyfikuje i profiluje swoją grupę docelową (pracownicy konkretnej firmy, branży, organizacji), następnie przeprowadza rekonesans by odkryć jakie strony członkowie tej grupy regularnie odwiedzają - mogą to być branżowe portale informacyjne, fora profesjonalne, lokalne strony restauracji, dostawców, strony organizacji branżowych. Kolejnym krokiem jest kompromitacja wybranej strony poprzez wykorzystanie jej podatności lub przejęcie kontroli nad nią. Atakujący umieszcza na zainfekowanej stronie złośliwy kod (często wykorzystujący exploit kit targetujący znane podatności przeglądarek, wtyczek Java, Flash) który aktywuje się gdy odwiedzi ją użytkownik ze spełniającymi warunki systemem. Często zastosowane są techniki fingerprinting by atakować tylko wybrane ofiary z konkretnej organizacji, a pozostałych użytkowników pozostawić nietknięte by wydłużyć czas wykrycia kompromitacji. Watering hole attacks są szczególnie skuteczne przeciwko organizacjom o wysokim poziomie bezpieczeństwa, które skutecznie bronią się przed phishingiem i innymi bezpośrednimi atakami. Są ulubioną taktyką grup APT i aktorów państwowych prowadzących kampanie szpiegowskie. Słynne przykłady to operacja targeting pracowników Apple i Facebooka przez zainfekowanie forum dla deweloperów iOS (2013), czy ataki na firmy energetyczne przez skompromitowane strony branżowe. Wykrywanie jest trudne ponieważ wykorzystywane strony są legitne i regularnie odwiedzane, a złośliwy kod może być aktywny tylko dla wybranych ofiar. Obrona wymaga: regularnego aktualizowania przeglądarek i wtyczek łatając exploity, wyłączenia niepotrzebnych wtyczek jak Flash i Java, wykorzystania sandboxed browsers dla przeglądania zewnętrznych stron, network segmentation ograniczającej rozprzestrzenianie się w przypadku infekcji, monitorowania anomalii w ruchu sieciowym (nietypowe połączenia do znanych stron, download plików wykonywalnych), threat intelligence identyfikującego kompromitacje popularnych stron w branży, okresowych audytów własnych stron często odwiedzanych przez partnerów, edukacji użytkowników o ryzyku, oraz stosowania principle of least privilege ograniczającego szkody od potencjalnej infekcji. Organizacje powinny także monitorować informacje o kompromitacjach stron w ich ekosystemie oraz mieć procedury szybkiego reagowania gdy ulubiona strona branżowa zostaje zainfekowana."
  },
  {
    "termin": "Threat Intelligence",
    "opis": "Threat Intelligence (pol. Wywiad o Zagrożeniach) to zbieranie, analiza, przetwarzanie i wymiana informacji o aktualnych i potencjalnych zagrożeniach cybernetycznych w celu umożliwienia organizacjom podejmowania świadomych decyzji dotyczących bezpieczeństwa. W przeciwieństwie do surowych danych o zagrożeniach (threat data), threat intelligence to informacje kontekstowe, actionable i istotne dla konkretnej organizacji. Threat intelligence odpowiada na kluczowe pytania: kto nas atakuje (aktorzy zagrożeń, ich motywacje, możliwości), co jest celem (jakie dane, systemy, sektory), jak prowadzą ataki (taktyki, techniki, procedury - TTP), kiedy i dlaczego (timing, kontekst geopolityczny). Threat intelligence dzieli się na kilka poziomów: Strategic Threat Intelligence (długoterminowy, wysokopoziomowy przegląd krajobrazu zagrożeń dla kadry zarządzającej, pomagający w decyzjach o inwestycjach i strategii), Tactical Threat Intelligence (informacje o TTPs używanych przez atakujących, dla zespołów bezpieczeństwa do ulepszania obron), Operational Threat Intelligence (informacje o konkretnych nadchodzących atakach lub kampaniach, pozwalające na proaktywną obronę), oraz Technical Threat Intelligence (techniczne wskaźniki kompromitacji - IOCs takie jak złośliwe IP, domeny, hash plików, dla natychmiastowej implementacji w zabezpieczeniach). Źródła threat intelligence są różnorodne: Open Source Intelligence - OSINT (publiczne źródła - fora, social media, raporty badaczy, CVE databases), Human Intelligence - HUMINT (informacje od ludzi - informatorzy, infiltracja), Technical Intelligence (analiza złośliwego oprogramowania, infrastruktury C&C), komercyjne platformy threat intelligence (płatne feedy dostarczane przez specjalistyczne firmy), Information Sharing and Analysis Centers - ISACs (branżowe organizacje wymiany informacji), oraz wewnętrzne źródła (logi, incydenty, honeypots). Cykl życia threat intelligence obejmuje: planning and direction (określenie wymagań i priorytetów), collection (zbieranie danych z różnych źródeł), processing (normalizacja, korelacja, wzbogacanie danych), analysis (wyciąganie wniosków, identyfikacja wzorców, attacker attribution), dissemination (dystrybucja intelligence do właściwych odbiorców w odpowiednim formacie), oraz feedback (ocena skuteczności i dostosowanie procesu). Standardy i frameworki wspierające threat intelligence to: STIX (Structured Threat Information Expression) - format wymiany informacji, TAXII (Trusted Automated Exchange of Intelligence Information) - protokół transportu, MITRE ATT&CK (matryca taktyk i technik), Diamond Model (framework analizy intruzji), Cyber Kill Chain (model faz ataku). Praktyczne zastosowania obejmują: wzbogacanie alertów SIEM kontekstem o aktorach i kampaniach, proaktywne blocking znanych złośliwych IOCs na firewallu, priorytetyzację łatania podatności eksploatowanych in-the-wild, threat hunting oparty na znanych TTPs grup, red teaming symulujący taktyki konkretnych grup relevant dla organizacji, strategic planning uwzględniający emerging threats. Wyzwania to false positives z przestarzałych IOCs, information overload wymagający skutecznej filtracji, trudność w attribution (przypisaniu ataków konkretnym aktorom), oraz konieczność kontekstualizacji generic intelligence do specyfiki organizacji. Skuteczny program threat intelligence wymaga jasno zdefiniowanych celów, dedykowanych zasobów, procesów analizy i dystrybucji, integracji z existing security tools, oraz kultury information sharing wewnątrz i na zewnątrz organizacji."
  },
  {
    "termin": "Atrybucja",
    "opis": "Atrybucja w kontekście cyberbezpieczeństwa to proces identyfikowania i przypisywania cyberataku konkretnemu aktorowi - osobie, grupie, organizacji lub państwu. Jest to jedno z najtrudniejszych wyzwań w dziedzinie cyberbezpieczeństwa ze względu na anonimową naturę internetu, łatwość fałszowania śladów i celowe działania atakujących mające na celu wprowadzenie w błąd (false flag operations). Atrybucja nie jest binarną odpowiedzią tak/nie, ale raczej oceną pewności opartą na dostępnych dowodach, często wyrażaną w terminach prawdopodobieństwa. Eksperci rozróżniają kilka poziomów atrybucji: technical attribution (powiązanie ataku z konkretną infrastrukturą techniczną - IP, domeny, malware), operational attribution (przypisanie do konkretnej grupy operacyjnej na podstawie TTPs), tactical attribution (identyfikacja kampanii i celów), oraz strategic attribution (określenie czy za atakiem stoi państwo i jakie są jego motywy geopolityczne). Proces atrybucji wykorzystuje różnorodne źródła dowodów: techniczne artefakty (złośliwe oprogramowanie, jego kod, kompilacja, język komentarzy, strefy czasowe aktywności), infrastruktura (serwery C&C, domeny, certyfikaty SSL, hosting), taktyki i techniki (charakterystyczne metody działania, narzędzia, procedury porównywane z known groups), cele ataków (branże, regiony, typy danych mogące sugerować motywacje), timing (korelacja z wydarzeniami geopolitycznymi, godziny pracy sugerujące strefy czasowe), błędy operacyjne atakujących (przypadkowe ujawnienia prawdziwych IP, ponowne użycie infrastruktury), językowe wskaźniki (język w malware, phishingu, błędy ortograficzne charakterystyczne dla native speakers), oraz HUMINT (ludzkie źródła, infiltracja grup). Frameworki wspierające atrybucję to Diamond Model of Intrusion Analysis mapujący relacje między adversary, capability, infrastructure i victim, oraz Q-Model oceniający quality i confidence w przypisaniu. Wyzwania w atrybucji są znaczące: łatwość użycia VPN, Tor, proxy chains maskujących prawdziwe źródło, false flag operations gdzie atakujący celowo podrzuca ślady sugerujące innego aktora (używanie narzędzi powiązanych z innymi grupami, fałszywe artefakty językowe), access brokers sprzedający dostęp do skompromitowanych systemów różnym klientom, shared tools wykorzystywane przez wiele grup, możliwość wynajęcia cyberprzestępców jako proxy przez państwa. Polityczne implikacje atrybucji są głębokie - publiczne przypisanie ataku państwu może prowadzić do napięć dyplomatycznych, sankcji, a nawet konfliktu. Dlatego standardy dowodów muszą być wysokie. US Intelligence Community używa skali confidence: low, moderate, high confidence w zależności od jakości i ilości dowodów. Przypadki publicznej atrybucji to: NotPetya przypisany Rosji przez US, UK i inne kraje, WannaCry przypisany Korei Północnej, szereg ataków APT przypisanych chińskim grupom państwowym. Organizacje prywatne rzadko dokonują pełnej strategic attribution, koncentrując się na tactical/operational poziomie wystarczającym do obrony. Dla skutecznej atrybucji kluczowa jest współpraca międzynarodowa, wymiana informacji między sektorami publicznym i prywatnym, oraz inwestycje w threat intelligence i cybercrime forensics. Warto pamiętać że brak atrybucji nie oznacza niemożności skutecznej obrony - organizacje mogą chronić się przed atakami niezależnie od wiedzy o atakującym, choć atrybucja pomaga w priorytetyzacji obron przeciwko najbardziej prawdopodobnym aktorom."
  },
  {
    "termin": "NIST 800-53",
    "opis": "NIST 800-53 (pełna nazwa: Security and Privacy Controls for Information Systems and Organizations) to kompleksowy katalog kontroli bezpieczeństwa i prywatności opublikowany przez National Institute of Standards and Technology, będący podstawą dla programów bezpieczeństwa wielu organizacji, szczególnie w sektorze publicznym USA oraz firm współpracujących z rządem federalnym. Dokument ten stanowi część szerszej rodziny standardów NIST Risk Management Framework (RMF) i jest regularnie aktualizowany by odzwierciedlać ewoluujący krajobraz zagrożeń - aktualna wersja Revision 5 została opublikowana w 2020 roku. NIST 800-53 zawiera ponad 1000 kontroli bezpieczeństwa i prywatności zorganizowanych w 20 rodzin: Access Control, Awareness and Training, Audit and Accountability, Assessment Authorization and Monitoring, Configuration Management, Contingency Planning, Identification and Authentication, Incident Response, Maintenance, Media Protection, Physical and Environmental Protection, Planning, Program Management, Personnel Security, PII Processing and Transparency, Risk Assessment, System and Services Acquisition, System and Communications Protection, System and Information Integrity, oraz Supply Chain Risk Management. Każda kontrola jest szczegółowo opisana z: control statement (co należy zaimplementować), supplemental guidance (jak to zrobić), control enhancements (opcjonalne wzmocnienia), related controls (powiązane kontrole), oraz references do innych standardów. Kontrole są podzielone na baselines odpowiadające poziomom wpływu według FIPS 199: Low, Moderate, High - organizacje wybierają baseline odpowiedni do wrażliwości swoich systemów i dostosowują go (tailoring) do swoich specyficznych potrzeb. Proces implementacji według RMF obejmuje: Prepare (przygotowanie organizacji i systemów), Categorize (kategoryzacja systemów według poziomu wpływu), Select (wybór odpowiedniego baseline kontroli), Implement (wdrożenie kontroli), Assess (ocena czy kontrole działają skutecznie), Authorize (formalna autoryzacja systemu przez decision maker), oraz Monitor (ciągłe monitorowanie efektywności kontroli). NIST 800-53 jest silnie powiązany z innymi standardami: NIST Cybersecurity Framework (CSF) mapuje do kontroli 800-53, NIST 800-171 (Protecting CUI in Nonfederal Systems) jest podzbiorem kontroli dla kontraktorów, FedRAMP wykorzystuje 800-53 dla autoryzacji cloud services. Rewizja 5 wprowadził znaczące zmiany: silniejszy focus na privacy controls, nowa rodzina Supply Chain Risk Management, kontrole zorientowane na outcomes nie implementacje, konsolidacja i uproszczenie wielu kontroli, większy nacisk na threat-informed defense. Dla organizacji spoza USA, NIST 800-53 często służy jako benchmark i źródło best practices nawet jeśli formalnie podlegają innym regulacjom (RODO, ISO 27001). Mapowania między NIST 800-53 a ISO 27001/27002 są publicznie dostępne ułatwiając organizacjom globalnym zarządzanie multiple compliance requirements. Implementacja pełnego NIST 800-53 baseline jest znaczącym przedsięwzięciem wymagającym zasobów, czasu i zaangażowania leadership, ale zapewnia comprehensive approach do cybersecurity i privacy. Organizacje mogą korzystać z automated compliance tools skanujących systemy i mapujących konfiguracje do wymagań 800-53, choć wiele kontroli (szczególnie proceduralne i administracyjne) wymaga ręcznej oceny. NIST udostępnia wszystkie publikacje za darmo, co czyni je dostępnym zasobem dla organizacji każdej wielkości chcących podnieść poziom swojego security posture."
  },
  {
    "termin": "Forensic Imaging",
    "opis": "Forensic Imaging (pol. Obrazowanie Kryminalistyczne) to proces tworzenia dokładnej, bit-po-bicie kopii nośnika danych (dysku twardego, SSD, pamięci flash, telefonu) w sposób zachowujący integralność dowodów dla celów dochodzenia kryminalistycznego lub analizy incydentów bezpieczeństwa. W przeciwieństwie do zwykłego backup, forensic image musi spełniać rygorystyczne standardy prawne i techniczne by być dopuszczalnym jako dowód w postępowaniach sądowych. Kluczowe zasady forensic imaging to: preservation (zachowanie oryginalnych dowodów w nienaruszonym stanie), documentation (szczegółowa dokumentacja każdego kroku procesu), chain of custody (udokumentowany łańcuch przechowywania dowodów), oraz repeatability (możliwość powtórzenia procesu z identycznymi wynikami). Proces obrazowania wykorzystuje specjalistyczne narzędzia hardware (write blockers fizycznie uniemożliwiające modyfikację źródłowego nośnika) i software (FTK Imager, EnCase, dd, dcfldd) tworzące perfect binary copy zawierającą nie tylko aktywne pliki, ale również deleted files w unallocated space, file slack, metadata, system files i partition information. Rodzaje obrazów to: physical image (bit-po-bicie kopia całego dysku włącznie z unallocated space), logical image (kopia tylko allocated files i folders), oraz sparse image (tylko używane sektory dla oszczędności miejsca). Formaty obrazów forensicznych to: raw/dd (prosty bit-po-bicie format), E01/Ex01 (Expert Witness Format używany przez EnCase z kompresją i metadata), AFF (Advanced Forensic Format otwarty standard), oraz proprietary formats różnych narzędzi. Każdy obraz forensiczny musi zawierać cryptographic hash (MD5, SHA-1, SHA-256) całego źródłowego nośnika i obrazu pozwalający na weryfikację integralności - jakiekolwiek modyfikacje zmienią hash potwierdzając kompromitację dowodu. Proces właściwego forensic imaging obejmuje: documentation stanu przed rozpoczęciem (fotografie, notatki), użycie write blockera, wybór odpowiedniego narzędzia imaging, utworzenie working copy (praca na kopii nigdy na oryginale), obliczenie i zapisanie hash values, secure storage oryginalnego nośnika, oraz szczegółowe logowanie wszystkich działań. Forensic imaging jest używane w: criminal investigations (przestępstwa komputerowe, fraud), civil litigation (spory o własność intelektualną), incident response (analiza naruszeń bezpieczeństwa), internal investigations (nadużycia pracowników), oraz e-discovery (postępowania prawne wymagające digital evidence). Współczesne wyzwania to rosnące rozmiary dysków (obrazowanie multi-terabyte drives jest czasochłonne), szyfrowanie (full disk encryption wymaga pozyskania kluczy przed obrazowaniem lub live imaging), cloud data (dane rozproszone w wielu lokalizacjach i jurisdykcjach), volatile memory (RAM imaging musi być wykonane na działającym systemie przed wyłączeniem), mobile devices (różnorodność platform i metod ekstrakcji), oraz BYOD (prawne i prywatne implikacje imaging osobistych urządzeń). Best practices obejmują używanie certyfikowanych narzędzi, training forensic examiners, documentation zgodna ze standardami (ISO 27037), secure evidence storage, oraz regular validation procedures. Memory (RAM) forensics zyskuje na znaczeniu dla wykrywania fileless malware i volatilnych artefaktów - narzędzia jak Volatility, Rekall czy MAGNET RAM Capture pozwalają na acquisition i analizę pamięci operacyjnej. Live forensics (imaging działającego systemu) jest czasem konieczne dla encrypted volumes ale może modyfikować system - wymaga specjalnych procedur minimalizujących wpływ. Organizacje powinny mieć incident response plans zawierające procedury forensic imaging, dostęp do narzędzi, przeszkolony personel oraz jasne protokoły kiedy i jak przeprowadzać imaging by zachować potencjalne dowody."
  }
  ]
